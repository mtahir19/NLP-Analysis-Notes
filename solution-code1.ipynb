{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP I: Language Data Pre-Processing and Sentiment Analysis\n",
    "\n",
    "_Authors: Matt Brems, Noelle Brown_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Define and implement tokenizing, lemmatizing, and stemming.\n",
    "2. Preprocess text data.\n",
    "3. Define and apply sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we begin, try running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get an error with the above code, run this & follow below directions:\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran into issues with the above:\n",
    "\n",
    "1. Run `nltk.download()`. A new screen will pop up outside your Jupyter notebook. (It may be hidden behind other windows.)\n",
    "2. Once this box opens up, click `all`, then `download`. Once this is done, restart your Jupyter notebook and try running the first three cells again.\n",
    "3. Run:\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")```\n",
    "\n",
    "    - If this returns `cat`, then fantastic! You’re done. \n",
    "    - If not, head to http://www.nltk.org/install.html and follow instructions for your computer, then try running the first three cells again.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural language processing** (NLP) describes the field of getting computers to understand language how we as humans do. Natural language processing has many, many applications including:\n",
    "\n",
    "- identifying trending topics on a social media website\n",
    "- automatically detecting spam\n",
    "- voice-to-text services\n",
    "- chatbots\n",
    "- translation\n",
    "\n",
    "Generally when we get text data, strings aren't broken out into individual words or even sentences. We might have a full tweet, full chapter of a book, or full .pdf file all in one long string.\n",
    "\n",
    "Today, we're diving into the practical side of NLP: taking text data and breaking it out into words that we can then leverage in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n"
     ]
    }
   ],
   "source": [
    "# Define spam text.\n",
    "spam = 'Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.'\n",
    "\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing \n",
    "\n",
    "When dealing with text data, there are common pre-processing steps. We won't necessarily use all of them every time we deal with text data.\n",
    "\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing special characters & Tokenizing\n",
    "\n",
    "We may need to remove unnecessary characters when cleaning text data (punctuation, symbols, etc.), depending on how we're going to use it. There are many ways to do this; we'll \n",
    "\n",
    "When we \"**tokenize**\" data, we take it and split it up into distinct chunks based on some pattern.\n",
    "\n",
    "Here we'll use a `RegexpTokenizer` do to these steps together.\n",
    "\n",
    "**Note**: Later we'll learn a different tool that we will use more frequently. It will allow us to preprocess, tokenize, and assemble the data in a way that scikit-learn will be able to easily handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,\\ni saw your contact information on linkedin.',\n",
       " 'i have carefully read through your profile and you seem to have an outstanding personality.',\n",
       " 'this is one major reason why i am in contact with you.',\n",
       " 'my name is mr. valery grayfer chairman of the board of directors of pjsc \"lukoil\".',\n",
       " 'i am 86 years old and i was diagnosed with cancer 2 years ago.',\n",
       " 'i will be going in for an operation later this week.',\n",
       " 'i decided to will/donate the sum of 8,750,000.00 euros(eight million seven hundred and fifty thousand euros only etc.',\n",
       " 'etc.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " ',',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " '.',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr.',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " '``',\n",
       " 'lukoil',\n",
       " \"''\",\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " '.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will/donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8,750,000.00',\n",
       " 'euros',\n",
       " '(',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'etc',\n",
       " '.',\n",
       " 'etc',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # We'll talk about this in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Run\" Tokenizer\n",
    "spam_tokens = tokenizer.tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'i', 'saw', 'your', 'contact', 'information', 'on', 'linkedin', 'i', 'have', 'carefully', 'read', 'through', 'your', 'profile', 'and', 'you', 'seem', 'to', 'have', 'an', 'outstanding', 'personality', 'this', 'is', 'one', 'major', 'reason', 'why', 'i', 'am', 'in', 'contact', 'with', 'you', 'my', 'name', 'is', 'mr', 'valery', 'grayfer', 'chairman', 'of', 'the', 'board', 'of', 'directors', 'of', 'pjsc', 'lukoil', 'i', 'am', '86', 'years', 'old', 'and', 'i', 'was', 'diagnosed', 'with', 'cancer', '2', 'years', 'ago', 'i', 'will', 'be', 'going', 'in', 'for', 'an', 'operation', 'later', 'this', 'week', 'i', 'decided', 'to', 'will', 'donate', 'the', 'sum', 'of', '8', '750', '000', '00', 'euros', 'eight', 'million', 'seven', 'hundred', 'and', 'fifty', 'thousand', 'euros', 'only', 'etc', 'etc']\n"
     ]
    }
   ],
   "source": [
    "# Show Results\n",
    "print(spam_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In comparing the original text to our tokenized version of the text, we converted one long string into a list of strings. What other changes occurred?</summary>\n",
    "\n",
    "- All strings were converted to lower case.\n",
    "- All punctuation was removed. (This was done using **regular expressions**.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly: Regular Expressions\n",
    "\n",
    "Regular Expressions, or RegEx, is a helpful tool for detecting patterns in text. \n",
    "\n",
    "RegEx is extremely powerful, and also extremely finicky. Many people use interactive tools to help them build their regular expressions.\n",
    "\n",
    "Do not use RegEx for [parsing HTML](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags)!\n",
    "\n",
    "When we instantiate `RegexpTokenizer(r'\\w+')`, the `\\w+` portion is the **regular expression** which tells Python to find \"one or more of any word character.\"\n",
    "\n",
    "Let's visit [RegEx101](https://regex101.com/) to investigate what that means.\n",
    "\n",
    "**Note**: For most of the work you do, you **will not need** regular expressions. RegEx is most useful when you need to find strings that match a certain variety of patterns, for example \"all things that might be phone numbers,\" \"all things that might be email addresses,\" or \"all things that might be social security numbers.\" It's important to know that RegEx is one tool you have, but using it here is overkill - we could get the same effect using inbuilt Python string methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/regex.png)\n",
    "\n",
    "[_from xkcd_](https://xkcd.com/1171/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lemmatizing & Stemming\n",
    "\n",
    "- \"He is *running* really fast!\"\n",
    "- \"He *ran* the race.\"\n",
    "- \"He *runs* a five-minute mile.\"\n",
    "\n",
    "If we wanted a computer to interpret these sentences, I might count up how many times I see each word. The computer will treat words like \"running,\" \"ran,\" and \"runs\" differently... but they mean very similar things (in this context)!\n",
    "\n",
    "**Lemmatizing** and **stemming** are two forms of shortening words so we can combine similar forms of the same word.\n",
    "\n",
    "When we \"**lemmatize**\" data, we take words and attempt to return their *lemma*, or the base/dictionary form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "tokens_lem = [lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'hello'),\n",
       " ('i', 'i'),\n",
       " ('saw', 'saw'),\n",
       " ('your', 'your'),\n",
       " ('contact', 'contact'),\n",
       " ('information', 'information'),\n",
       " ('on', 'on'),\n",
       " ('linkedin', 'linkedin'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('carefully', 'carefully'),\n",
       " ('read', 'read'),\n",
       " ('through', 'through'),\n",
       " ('your', 'your'),\n",
       " ('profile', 'profile'),\n",
       " ('and', 'and'),\n",
       " ('you', 'you'),\n",
       " ('seem', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('have', 'have'),\n",
       " ('an', 'an'),\n",
       " ('outstanding', 'outstanding'),\n",
       " ('personality', 'personality'),\n",
       " ('this', 'this'),\n",
       " ('is', 'is'),\n",
       " ('one', 'one'),\n",
       " ('major', 'major'),\n",
       " ('reason', 'reason'),\n",
       " ('why', 'why'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('in', 'in'),\n",
       " ('contact', 'contact'),\n",
       " ('with', 'with'),\n",
       " ('you', 'you'),\n",
       " ('my', 'my'),\n",
       " ('name', 'name'),\n",
       " ('is', 'is'),\n",
       " ('mr', 'mr'),\n",
       " ('valery', 'valery'),\n",
       " ('grayfer', 'grayfer'),\n",
       " ('chairman', 'chairman'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('board', 'board'),\n",
       " ('of', 'of'),\n",
       " ('directors', 'director'),\n",
       " ('of', 'of'),\n",
       " ('pjsc', 'pjsc'),\n",
       " ('lukoil', 'lukoil'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('86', '86'),\n",
       " ('years', 'year'),\n",
       " ('old', 'old'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnosed'),\n",
       " ('with', 'with'),\n",
       " ('cancer', 'cancer'),\n",
       " ('2', '2'),\n",
       " ('years', 'year'),\n",
       " ('ago', 'ago'),\n",
       " ('i', 'i'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('going', 'going'),\n",
       " ('in', 'in'),\n",
       " ('for', 'for'),\n",
       " ('an', 'an'),\n",
       " ('operation', 'operation'),\n",
       " ('later', 'later'),\n",
       " ('this', 'this'),\n",
       " ('week', 'week'),\n",
       " ('i', 'i'),\n",
       " ('decided', 'decided'),\n",
       " ('to', 'to'),\n",
       " ('will', 'will'),\n",
       " ('donate', 'donate'),\n",
       " ('the', 'the'),\n",
       " ('sum', 'sum'),\n",
       " ('of', 'of'),\n",
       " ('8', '8'),\n",
       " ('750', '750'),\n",
       " ('000', '000'),\n",
       " ('00', '00'),\n",
       " ('euros', 'euro'),\n",
       " ('eight', 'eight'),\n",
       " ('million', 'million'),\n",
       " ('seven', 'seven'),\n",
       " ('hundred', 'hundred'),\n",
       " ('and', 'and'),\n",
       " ('fifty', 'fifty'),\n",
       " ('thousand', 'thousand'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'only'),\n",
       " ('etc', 'etc'),\n",
       " ('etc', 'etc')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare tokens to lemmatized version.\n",
    "list(zip(spam_tokens, tokens_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('directors', 'director')\n",
      "('years', 'year')\n",
      "('was', 'wa')\n",
      "('years', 'year')\n",
      "('euros', 'euro')\n",
      "('euros', 'euro')\n"
     ]
    }
   ],
   "source": [
    "# Print only those lemmatized tokens that are different.\n",
    "for i in range(len(spam_tokens)):\n",
    "    if spam_tokens[i] != tokens_lem[i]:\n",
    "        print((spam_tokens[i], tokens_lem[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directors', 'director'),\n",
       " ('years', 'year'),\n",
       " ('was', 'wa'),\n",
       " ('years', 'year'),\n",
       " ('euros', 'euro'),\n",
       " ('euros', 'euro')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(spam_tokens[i], tokens_lem[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != tokens_lem[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing is sometimes very useful and sometimes makes the task at hand harder.\n",
    "\n",
    "As an example, imagine searching a database of academic papers for the string \"democracy.\" We would probably be interested in seeing papers on \"democratically elected governments,\" \"democratic trends,\" and even \"democracies.\" It might be appropriate for the database search engine to lemmatize the query and match against lemmatized paper titles.\n",
    "\n",
    "Let's try a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computers.\"\n",
    "lemmatizer.lemmatize(\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computation'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computationally'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "When we \"**stem**\" data, we take words and attempt to return a base form of the word. It tends to be cruder than using lemmatization. There's a [method developed by Porter in 1980](https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf) that explains the algorithm used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PorterStemmer.\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem tokens.\n",
    "stem_spam = [p_stemmer.stem(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'hello'),\n",
       " ('i', 'i'),\n",
       " ('saw', 'saw'),\n",
       " ('your', 'your'),\n",
       " ('contact', 'contact'),\n",
       " ('information', 'inform'),\n",
       " ('on', 'on'),\n",
       " ('linkedin', 'linkedin'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('carefully', 'care'),\n",
       " ('read', 'read'),\n",
       " ('through', 'through'),\n",
       " ('your', 'your'),\n",
       " ('profile', 'profil'),\n",
       " ('and', 'and'),\n",
       " ('you', 'you'),\n",
       " ('seem', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('have', 'have'),\n",
       " ('an', 'an'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('is', 'is'),\n",
       " ('one', 'one'),\n",
       " ('major', 'major'),\n",
       " ('reason', 'reason'),\n",
       " ('why', 'whi'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('in', 'in'),\n",
       " ('contact', 'contact'),\n",
       " ('with', 'with'),\n",
       " ('you', 'you'),\n",
       " ('my', 'my'),\n",
       " ('name', 'name'),\n",
       " ('is', 'is'),\n",
       " ('mr', 'mr'),\n",
       " ('valery', 'valeri'),\n",
       " ('grayfer', 'grayfer'),\n",
       " ('chairman', 'chairman'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('board', 'board'),\n",
       " ('of', 'of'),\n",
       " ('directors', 'director'),\n",
       " ('of', 'of'),\n",
       " ('pjsc', 'pjsc'),\n",
       " ('lukoil', 'lukoil'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('86', '86'),\n",
       " ('years', 'year'),\n",
       " ('old', 'old'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('with', 'with'),\n",
       " ('cancer', 'cancer'),\n",
       " ('2', '2'),\n",
       " ('years', 'year'),\n",
       " ('ago', 'ago'),\n",
       " ('i', 'i'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('going', 'go'),\n",
       " ('in', 'in'),\n",
       " ('for', 'for'),\n",
       " ('an', 'an'),\n",
       " ('operation', 'oper'),\n",
       " ('later', 'later'),\n",
       " ('this', 'thi'),\n",
       " ('week', 'week'),\n",
       " ('i', 'i'),\n",
       " ('decided', 'decid'),\n",
       " ('to', 'to'),\n",
       " ('will', 'will'),\n",
       " ('donate', 'donat'),\n",
       " ('the', 'the'),\n",
       " ('sum', 'sum'),\n",
       " ('of', 'of'),\n",
       " ('8', '8'),\n",
       " ('750', '750'),\n",
       " ('000', '000'),\n",
       " ('00', '00'),\n",
       " ('euros', 'euro'),\n",
       " ('eight', 'eight'),\n",
       " ('million', 'million'),\n",
       " ('seven', 'seven'),\n",
       " ('hundred', 'hundr'),\n",
       " ('and', 'and'),\n",
       " ('fifty', 'fifti'),\n",
       " ('thousand', 'thousand'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli'),\n",
       " ('etc', 'etc'),\n",
       " ('etc', 'etc')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare tokens to stemmed version.\n",
    "list(zip(spam_tokens, stem_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('information', 'inform'),\n",
       " ('carefully', 'care'),\n",
       " ('profile', 'profil'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('why', 'whi'),\n",
       " ('valery', 'valeri'),\n",
       " ('directors', 'director'),\n",
       " ('years', 'year'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('years', 'year'),\n",
       " ('going', 'go'),\n",
       " ('operation', 'oper'),\n",
       " ('this', 'thi'),\n",
       " ('decided', 'decid'),\n",
       " ('donate', 'donat'),\n",
       " ('euros', 'euro'),\n",
       " ('hundred', 'hundr'),\n",
       " ('fifty', 'fifti'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print only those stemmed tokens that are different.\n",
    "\n",
    "[(spam_tokens[i], stem_spam[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != stem_spam[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this on individual words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computers\"\n",
    "p_stemmer.stem(\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computer.\"\n",
    "p_stemmer.stem(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computation.\"\n",
    "p_stemmer.stem(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computationally.\"\n",
    "p_stemmer.stem(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following quote has had stop words (and punctuation) removed:\n",
    "\n",
    "\"Answer great question life universe everything said deep thought said deep thought paused forty two said deep thought infinite majesty calm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What book is the above sentence from?</summary>\n",
    "\n",
    "The Hitchhiker's Guide to the Galaxy!\n",
    "    \n",
    "![](../images/hgg.jpg)\n",
    "    \n",
    "The original quote reads:  \n",
    "...\"The Answer to the Great Question...\"  \n",
    "\"Yes..!\"  \n",
    "\"Of Life, the Universe and Everything...\" said Deep Thought.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\" said Deep Thought, and paused.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\"  \n",
    "\"Yes...!!!...?\"  \n",
    "\"Forty-two,\" said Deep Thought, with infinite majesty and calm.”\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If you were familiar with the book, how did you know what book the sentence was from?</summary>\n",
    "\n",
    "Removing stop words did not remove key identifying words such as \"life\", \"universe\", \"everything\", and \"forty-two\".\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Based on this, how would you define stop words?</summary>\n",
    "\n",
    "Stop words are words that have little to no significance or meaning. They are common words that only add to the grammatical structure and flow of the sentence, so it is still relatively easy to identify the contents of sentences without stop words.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print English stopwords.\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from \"spam_tokens.\"\n",
    "no_stop_words = [token for token in spam_tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'saw', 'contact', 'information', 'linkedin', 'carefully', 'read', 'profile', 'seem', 'outstanding', 'personality', 'one', 'major', 'reason', 'contact', 'name', 'mr', 'valery', 'grayfer', 'chairman', 'board', 'directors', 'pjsc', 'lukoil', '86', 'years', 'old', 'diagnosed', 'cancer', '2', 'years', 'ago', 'going', 'operation', 'later', 'week', 'decided', 'donate', 'sum', '8', '750', '000', '00', 'euros', 'eight', 'million', 'seven', 'hundred', 'fifty', 'thousand', 'euros', 'etc', 'etc']\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "print(no_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "![](../images/sent.jpeg)\n",
    "\n",
    "[Sentiment analysis](https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html) is an area of natural language processing in which we seek to classify text as having positive or negative emotion.\n",
    "\n",
    "We'll look today at the [VADER sentiment analyzer](https://github.com/cjhutto/vaderSentiment). Vader stands for \"valence-aware dictionary and sentiment reasoner.\" We won't have to download anything new; NLTK comes with VADER. You can read the [VADER paper](https://www.scinapse.io/papers/2099813784#fullText) (pdf link) for more details on how VADER was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = '''Took a chance on this blouse and so glad i did. i wasn't crazy about how the blouse is photographed on the model. i paired it whit white pants and it worked perfectly. crisp and clean is how i would describe it. launders well. fits great. drape is perfect. wear tucked in or out - can't go wrong.'''\n",
    "\n",
    "review_2 = '''First of all, this is not pullover styling. there is a side zipper. i wouldn't have purchased it if i knew there was a side zipper because i have a large bust and side zippers are next to impossible for me.\n",
    "\n",
    "second of all, the tulle feels and looks cheap and the slip has an awkward tight shape underneath.\n",
    "\n",
    "not at all what is looks like or is described as. sadly will be returning, but i'm sure i will find something to exchange it for!'''\n",
    "\n",
    "review_3 = '''I don't normally review my purchases, but i was so amazed at how poorly this dress was made, i couldn't help myself but to post a review. the neck line isn't even hemmed down so it flaps up. the material is thin and feel cheap. this dress isnt even worth $20 in my opinion. i was expecting a well made, good quality dress for the high price tag.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took a chance on this blouse and so glad i did. i wasn't crazy about how the blouse is photographed on the model. i paired it whit white pants and it worked perfectly. crisp and clean is how i would describe it. launders well. fits great. drape is perfect. wear tucked in or out - can't go wrong.\n"
     ]
    }
   ],
   "source": [
    "print(review_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First of all, this is not pullover styling. there is a side zipper. i wouldn't have purchased it if i knew there was a side zipper because i have a large bust and side zippers are next to impossible for me.\n",
      "\n",
      "second of all, the tulle feels and looks cheap and the slip has an awkward tight shape underneath.\n",
      "\n",
      "not at all what is looks like or is described as. sadly will be returning, but i'm sure i will find something to exchange it for!\n"
     ]
    }
   ],
   "source": [
    "print(review_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't normally review my purchases, but i was so amazed at how poorly this dress was made, i couldn't help myself but to post a review. the neck line isn't even hemmed down so it flaps up. the material is thin and feel cheap. this dress isnt even worth $20 in my opinion. i was expecting a well made, good quality dress for the high price tag.\n"
     ]
    }
   ],
   "source": [
    "print(review_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'compound': 0.9782}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_1.\n",
    "sia.polarity_scores(review_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.039, 'neu': 0.9, 'pos': 0.061, 'compound': 0.4199}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_2.\n",
    "sia.polarity_scores(review_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.066, 'neu': 0.77, 'pos': 0.164, 'compound': 0.8515}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_3.\n",
    "sia.polarity_scores(review_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What limitations have you noticed?\n",
    "\n",
    "Let's try a few random examples ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.328, 'pos': 0.672, 'compound': 0.6249}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('this is awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.293, 'pos': 0.707, 'compound': 0.7034}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('this is AWESOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4588}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(':)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4404}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(':(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the whole reviews dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.099103Z",
     "start_time": "2017-10-24T15:40:05.659674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in training data.\n",
    "reviews = pd.read_csv(\"../data/womens-clothing-reviews.csv\")[['Review Text', 'Rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.680295Z",
     "start_time": "2017-10-24T15:40:07.659485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  Rating\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4\n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5\n",
       "2  I had such high hopes for this dress and reall...       3\n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5\n",
       "4  This shirt is very flattering to all due to th...       5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first five rows.\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine a review.\n",
    "reviews['Review Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.027, 'neu': 0.792, 'pos': 0.181, 'compound': 0.9427}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment of the review.\n",
    "sia.polarity_scores(reviews['Review Text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does this match the sentiment given in the training data?\n",
    "reviews['Rating'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>When processing text, what can you do about frequently occurring words, like \"a,\" \"and,\" \"the,\" etc.?</summary>\n",
    "\n",
    "- These words, called \"stopwords,\" can either be kept or removed.\n",
    "    - If we think these words do help explain our $Y$ variable, we might keep them. (For example, if we're classifying the era of a poem, the frequency of the word \"the\" may be helpful information!)\n",
    "    - If we think these words don't help explain our $Y$ variable, we might remove them. (For example, in sentiment analysis, we might not think that people who use \"the\" more or less frequently are happier or angrier.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note:\n",
    "1. NLP broadly describes: \n",
    "    - how we can get unstructured text data into a more structured form that can be interpreted by computers, and \n",
    "    - algorithms for interpreting text data.\n",
    "2. That does not mean these tools we used today work to the exclusion of other methods. You can and should include other variables in your model!\n",
    "    - For example, maybe the length of a review tells us something about how much people liked/disliked the movie, or maybe additional information about the reviewer (i.e. geography, age, how many reviews they had submitted) has predictive value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 226,
   "position": {
    "height": "40px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
